{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4918f5",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "This note explores how data is prepared for BERT, which is trained in two distinct phases using two different data pipelines.\n",
    "\n",
    "1.  **Pretraining:** The model learns \"language\" (grammar, context, facts) from massive unlabeled text corpora.\n",
    "2.  **Finetuning:** The model adapts its knowledge to solve a specific task (e.g., Sentiment Analysis, Question Answering, etc.) using labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709eb89",
   "metadata": {},
   "source": [
    "## Phase 1: Pretraining\n",
    "\n",
    "Pretraining is \"unsupervised\" (or self-supervised). We don't have human labels. Instead, we generate labels from the text itself.\n",
    "\n",
    "### Task A: Next Sentence Prediction (NSP)\n",
    "BERT needs to understand the relationship *between* sentences to handle tasks like Question Answering (where the answer follows the question). For this, we feed the model with the following input:\n",
    "\n",
    "`[CLS] Sentence A [SEP] Sentence B [SEP]`\n",
    "\n",
    "and:\n",
    "* **50% of the time:** `Sentence B` is the actual sentence next to `Sentence A`. Generate label accordingly for `Sentence B`: `IS_NEXT`.\n",
    "* **50% of the time:** `Sentence B` is a random sentence from the corpus. Genrate label: `NOT_NEXT`.\n",
    "\n",
    "This forces the model to look at the `[CLS]` token embedding and decide if the two sentences flow logically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc11584",
   "metadata": {},
   "source": [
    "### Task B: Masked Language Modeling (MLM)\n",
    "Standard language models (like GPT) predict the *next* token (left-to-right). This limits context.\\\n",
    "BERT is **Bidirectional**: it looks at the whole sentence at once. To prevent it from \"cheating\" and seeing the answer, we hide / mask some tokens.\n",
    "\n",
    "1.  Select **15%** of tokens to mask.\n",
    "2.  Apply the **80-10-10 Rule** to these selected tokens:\n",
    "    * **80%:** Replace with `[MASK]`. The model must predict the original token based on context.\n",
    "    * **10%:** Replace with a **Random Token**. This forces the model to keep checking the input (it can't just trust that non-masked tokens are correct).\n",
    "    * **10%:** Keep the **Original Token**. This biases the model towards the real token so it doesn't always predict \"the input is wrong.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5fa64f",
   "metadata": {},
   "source": [
    "Let's see both of these data pipeline tasks in action.\\\n",
    "As before, let's start by adding _src_ to the python system path in case your notebook is not being run with it already added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76dc1c7-f9bd-41ce-8d6f-4859c88c9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append((Path('').resolve().parent / 'src').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827eeb1-a1fd-423a-a52c-6c0b76e7fc2b",
   "metadata": {},
   "source": [
    "We will start by training a BPE tokenizer with our super fast Rust implementation on the same [wikitext](https://huggingface.co/datasets/Salesforce/wikitext) available in Hugging Face that we used in the tokenizetion notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32959611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import TokenizerSettings\n",
    "from token_encoders.rust.bpe import RustBPETokenizer\n",
    "\n",
    "corpus_file = Path(\"\").resolve().parent / \"data\"/ \"wikitext_103.txt\"\n",
    "text = corpus_file.read_text()\n",
    "\n",
    "tokenizer = RustBPETokenizer(TokenizerSettings())\n",
    "tokenizer.train([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcd08b-adc8-41b3-82f6-ff146dda25ab",
   "metadata": {},
   "source": [
    "Now that we have a tokenizer, let's create some dummy documents to see how NSP and MLM affect the data. In practice we will be loading these documents from a file, but here to illustrate we will just define them as variables. If you want to take a look more in details into implementations, you can go to the _src/datasets_ module, where you will find everything related to pretraining, finetuning and inference data corpora, dataloaders and datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf82bf82-6f79-40dc-8d39-28c55d74610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Initialized. Total samples available: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from datasets.pretraining import NSPLabel, PretrainingDataset\n",
    "from datasets.types.inputs.pretraining import PretrainingCorpusData\n",
    "from settings import LoaderSettings\n",
    "\n",
    "documents = [\n",
    "    # Document 1: Artificial Intelligence & BERT (Logical Explanation Flow)\n",
    "    [\n",
    "        \"BERT models are pretrained on large text corpora to understand language.\",\n",
    "        \"They use masked language modeling to learn context from these texts.\",\n",
    "        \"In this process, random words are hidden and the model guesses them.\",\n",
    "        \"Next sentence prediction helps with logical coherence between clauses.\",\n",
    "        \"This requires the model to understand if two sentences belong together.\",\n",
    "        \"Transformer architectures rely heavily on self-attention mechanisms to do this.\",\n",
    "        \"Attention heads allow the model to focus on different parts of a sentence.\",\n",
    "        \"The bidirectional nature of BERT provides a deeper understanding than previous models.\",\n",
    "        \"Fine-tuning allows these pretrained models to adapt to specific problems.\",\n",
    "        \"Deep learning has thus revolutionized natural language processing tasks.\"\n",
    "    ],\n",
    "    # Document 2: Baking & Cooking (Chronological Steps)\n",
    "    [\n",
    "        \"To bake a perfect cake you need fresh flour and sugar.\",\n",
    "        \"First, mix the dry ingredients together in a large ceramic bowl.\",\n",
    "        \"Next, whisk the eggs and butter until the mixture is smooth.\",\n",
    "        \"Pour the wet mixture into the dry bowl and stir gently.\",\n",
    "        \"Preheat your oven to 350 degrees before you start baking.\",\n",
    "        \"Grease the baking pan thoroughly to prevent the cake from sticking.\",\n",
    "        \"Pour the batter evenly into the prepared cake tin.\",\n",
    "        \"Bake in the center of the oven for approximately thirty minutes.\",\n",
    "        \"Insert a toothpick to check if the cake is fully cooked inside.\",\n",
    "        \"Finally, let the cake cool on a wire rack before applying frosting.\"\n",
    "    ],\n",
    "    # Document 3: Space Exploration (Spatial Flow: Earth -> Outwards)\n",
    "    [\n",
    "        \"Astronomy begins with observing the sky from our home planet.\",\n",
    "        \"The moon is Earth's only natural satellite and the closest celestial body.\",\n",
    "        \"Beyond the moon, our solar system consists of eight planets orbiting the sun.\",\n",
    "        \"Mars is the next target for human exploration due to its proximity.\",\n",
    "        \"Future missions aim to land humans on the Red Planet within a decade.\",\n",
    "        \"Further out, gas giants like Saturn display spectacular ring systems.\",\n",
    "        \"Our sun is just one of billions of stars in the Milky Way galaxy.\",\n",
    "        \"Stars are born in giant clouds of gas and dust called nebulae.\",\n",
    "        \"When massive stars die, they can collapse into black holes.\",\n",
    "        \"The universe is vast, expanding, and filled with infinite mysteries.\"\n",
    "    ],\n",
    "    # Document 4: Oceanography (Depth Flow: Surface -> Deep Sea)\n",
    "    [\n",
    "        \"The ocean covers more than seventy percent of the Earth's surface.\",\n",
    "        \"The surface is shaped by tides caused by the moon's gravity.\",\n",
    "        \"Ocean currents regulate the global climate by transporting this heat.\",\n",
    "        \"Just below the surface, plankton serves as the base of the food web.\",\n",
    "        \"Coral reefs thrive in these shallow waters, hosting diverse ecosystems.\",\n",
    "        \"Larger animals like whales migrate thousands of miles through these waters.\",\n",
    "        \"As we go deeper, sunlight fades and the water becomes much colder.\",\n",
    "        \"Deep sea trenches contain some of the most mysterious creatures on Earth.\",\n",
    "        \"Many species in this dark abyss generate their own light through bioluminescence.\",\n",
    "        \"Exploring this ocean floor is as difficult as exploring outer space.\"\n",
    "    ],\n",
    "    # Document 5: History of Civilization (Chronological Timeline)\n",
    "    [\n",
    "        \"Early humans lived as hunter-gatherers, moving constantly to find food.\",\n",
    "        \"The discovery of agriculture allowed these groups to settle in one place.\",\n",
    "        \"Settlements grew into cities, requiring new ways to organize society.\",\n",
    "        \"Ancient civilizations like Egypt developed writing to record their history.\",\n",
    "        \"Later, the Greek empire laid the foundation for Western philosophy and democracy.\",\n",
    "        \"The Roman Empire expanded these ideas across a vast network of roads.\",\n",
    "        \"After Rome fell, the Middle Ages brought a period of feudalism.\",\n",
    "        \"The Renaissance later sparked a rebirth of art, science, and exploration.\",\n",
    "        \"The Industrial Revolution eventually changed how humans lived and worked forever.\",\n",
    "        \"Today, we live in a globalized world connected by digital technology.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "corpus_data = PretrainingCorpusData(documents=documents)\n",
    "loader_settings = LoaderSettings(max_seq_len=48, batch_size=2, shuffle=True)\n",
    "ds = PretrainingDataset(corpus_data, tokenizer, loader_settings)\n",
    "print(f\"Dataset Initialized. Total samples available: {len(ds)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c90300-676c-4394-9ada-f16b0105935e",
   "metadata": {},
   "source": [
    "Let's pick a random sentence, and see how NSP and MLM affect it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433d36b8-3296-4f0a-83ad-e1b35f6cd93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Analysis (Index 6) ---\n",
      "Original sentence:\n",
      "Attention heads allow the model to focus on different parts of a sentence.\n",
      "\n",
      "[1] NSP Label: 1 (NOT_NEXT)\n",
      "Input Sequence:\n",
      "'[CLS] Attention[MASK] allow the model to focus on different[MASK] of a sentence.[SEP] Deep sea trenches contain some ing the most mysterious creatures on Earth.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'\n",
      "\n",
      "[2] MLM Analysis (Comparing Input vs Labels)\n",
      "Input Token          | Target Label         | Status\n",
      "-----------------------------------------------------------------\n",
      "[CLS]                | -                    | Ignored\n",
      "ĠAtt                 | -                    | Ignored\n",
      "ention               | -                    | Ignored\n",
      "[MASK]               | Ġheads               | MASKED [80%]\n",
      "Ġallow               | -                    | Ignored\n",
      "Ġthe                 | -                    | Ignored\n",
      "Ġmodel               | -                    | Ignored\n",
      "Ġto                  | -                    | Ignored\n",
      "Ġfocus               | -                    | Ignored\n",
      "Ġon                  | -                    | Ignored\n",
      "Ġdifferent           | -                    | Ignored\n",
      "[MASK]               | Ġparts               | MASKED [80%]\n",
      "Ġof                  | -                    | Ignored\n",
      "Ġa                   | -                    | Ignored\n",
      "Ġsentence            | -                    | Ignored\n",
      ".                    | -                    | Ignored\n",
      "[SEP]                | -                    | Ignored\n",
      "ĠDeep                | -                    | Ignored\n",
      "Ġsea                 | -                    | Ignored\n",
      "Ġtrenches            | -                    | Ignored\n",
      "Ġcontain             | -                    | Ignored\n",
      "Ġsome                | -                    | Ignored\n",
      "Ġing                 | Ġof                  | RANDOM WORD [10%]\n",
      "Ġthe                 | -                    | Ignored\n",
      "Ġmost                | -                    | Ignored\n",
      "Ġmysterious          | -                    | Ignored\n",
      "Ġcreatures           | -                    | Ignored\n",
      "Ġon                  | -                    | Ignored\n",
      "ĠEarth               | ĠEarth               | UNCHANGED [10%]\n",
      ".                    | -                    | Ignored\n",
      "[SEP]                | -                    | Ignored\n"
     ]
    }
   ],
   "source": [
    "sample_idx = random.randint(0, len(ds) - 1)\n",
    "print(f\"--- Sample Analysis (Index {sample_idx}) ---\")\n",
    "doc_idx, sent_idx = ds.samples_index[sample_idx]\n",
    "print(f\"Original sentence:\\n{ds.data[doc_idx][sent_idx]}\")\n",
    "\n",
    "sample = ds[sample_idx]\n",
    "label_name = NSPLabel(sample.nsp_labels.item()).name\n",
    "print(f\"\\n[1] NSP Label: {sample.nsp_labels.item()} ({label_name})\")\n",
    "token_ids = sample.input_ids.tolist()\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"Input Sequence:\\n'{decoded_text}'\")\n",
    "print(f\"\\n[2] MLM Analysis (Comparing Input vs Labels)\")\n",
    "print(f\"{'Input Token':<20} | {'Target Label':<20} | {'Status'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "labels = sample.mlm_labels.tolist()\n",
    "\n",
    "\n",
    "for i, (inp_id, lbl_id) in enumerate(zip(token_ids, labels)):\n",
    "    if inp_id == tokenizer.pad_token_id:\n",
    "        continue\n",
    "    inp_token = tokenizer.inverse_vocab.get(inp_id, f\"[{inp_id}]\")\n",
    "    if lbl_id == -100:\n",
    "        lbl_token = \"-\"\n",
    "        status = \"Ignored\"\n",
    "    else:\n",
    "        lbl_token = tokenizer.inverse_vocab.get(lbl_id, f\"[{lbl_id}]\")\n",
    "        if inp_id == tokenizer.mask_token_id:\n",
    "            status = \"MASKED [80%]\"\n",
    "        elif inp_id != lbl_id:\n",
    "            status = \"RANDOM WORD [10%]\"\n",
    "        else:\n",
    "            status = \"UNCHANGED [10%]\"\n",
    "    print(f\"{inp_token:<20} | {lbl_token:<20} | {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97533fbf",
   "metadata": {},
   "source": [
    "## Phase 2: Finetuning\n",
    "\n",
    "Once the model understands language (via Pretraining), we adapt it.\n",
    "Finetuning is much simpler:\n",
    "1.  **Input:** Single sequence (usually).\n",
    "2.  **Output:** A specific label (Sentiment 0/1, Category A/B/C).\n",
    "3.  **No Masking:** We want the model to see the whole text.\n",
    "4.  **No Random Next Sentence:** The input is just the document we want to classify.\n",
    "\n",
    "We usually take the embedding of the `[CLS]` token from the last layer and pass it through a simple classifier layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
