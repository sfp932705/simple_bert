{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2b268f",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "The **Embedding Layer** is the bridge between raw integers (token IDs produced by tokenizers) and the vector space where the model operates.\\\n",
    "In standard NLP models (like Word2Vec), this is just a single lookup table. In BERT, it is a sum of three distinct embeddings.\n",
    "\n",
    "1.  **Token Embeddings:** The meaning of the word itself (e.g., \"bank\" $\\to$ vector).\n",
    "2.  **Position Embeddings:** The position in the sentence (e.g., Position 0 vs Position 5).\n",
    "    * *Why?* The Transformer architecture (Attention) is \"permutation invariant.\" It has no inherent sense of order. If we didn't add this, \"Man bites dog\" and \"Dog bites man\" would look identical to the model.\n",
    "3.  **Token Type (Segment) Embeddings:** Indicates which sentence the token belongs to.\n",
    "    * *Why?* BERT is trained on pairs. `Sentence A` gets ID 0, `Sentence B` gets ID 1.\n",
    "\n",
    "The final representation for a token is:\n",
    "$$ E_{\\text{final}} = E_{\\text{token}} + E_{\\text{position}} + E_{\\text{type}} $$\n",
    "\n",
    "We use element-wise **addition**, not concatenation. This keeps the vector size constant (e.g., 768) throughout the model.\n",
    "\n",
    "Let's start by adding _src_ to the python system path in case your notebook is not being run with it already added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f9a704-72c5-4b55-9236-df2d3b154164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append((Path('').resolve().parent / 'src').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f78bd-4fbd-457f-970f-190559f52620",
   "metadata": {},
   "source": [
    "### Initializing the Layer\n",
    "Let's verify the structure of the module. You will see the three distinct `nn.Embedding` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734b63a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings(\n",
      "  (word_embeddings): Embedding(120, 16, padding_idx=0)\n",
      "  (position_embeddings): Embedding(50, 16)\n",
      "  (token_type_embeddings): Embedding(2, 16)\n",
      "  (LayerNorm): LayerNorm((16,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "Token Embeddings:         torch.Size([120, 16]) (Vocab x Hidden)\n",
      "Positional Embeddings:    torch.Size([50, 16]) (MaxPos x Hidden)\n",
      "Type Embeddings:          torch.Size([2, 16])  (Types x Hidden)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from modules.embeddings import Embeddings\n",
    "from settings import EmbeddingSettings\n",
    "\n",
    "settings = EmbeddingSettings(\n",
    "    vocab_size=120,\n",
    "    hidden_size=16,           # Small vector size for readability\n",
    "    max_position_embeddings=50,\n",
    "    type_vocab_size=2,\n",
    "    hidden_dropout_prob=0.0   # Disable dropout for deterministic results\n",
    ")\n",
    "\n",
    "embeddings_layer = Embeddings(settings)\n",
    "print(embeddings_layer)\n",
    "\n",
    "\n",
    "print(f\"\\n{'Token Embeddings:':<25} {str(embeddings_layer.word_embeddings.weight.shape):<20} (Vocab x Hidden)\")\n",
    "print(f\"{'Positional Embeddings:':<25} {str(embeddings_layer.position_embeddings.weight.shape):<20} (MaxPos x Hidden)\")\n",
    "print(f\"{'Type Embeddings:':<25} {str(embeddings_layer.token_type_embeddings.weight.shape):<20} (Types x Hidden)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a993c",
   "metadata": {},
   "source": [
    "### The Forward Pass (Summation)\n",
    "\n",
    "We will pass a batch of sentences.\n",
    "* **Batch Size:** 2\n",
    "* **Max Sequence Length in batch:** 5\n",
    "* **Sentences:** `[10, 20, 30]` and `[25, 40, 42, 22, 33]`\n",
    "\n",
    "The first sentence ends up being padded to length 5, resulting in `[10, 20, 30, 0, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720e94ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([2, 5])\n",
      "Output Shape: torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([[10, 20, 30, 0, 0], [25, 40, 42, 22, 33]])\n",
    "# Note: We are NOT passing position_ids manually. The model generates them automatically.\n",
    "output = embeddings_layer(input_ids)\n",
    "print(f\"Input Shape:  {input_ids.shape}\")\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "# Expected: (2, 5, 16) -> (Batch, Seq, Hidden)\n",
    "assert output.shape == (2, 5, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0fcae0",
   "metadata": {},
   "source": [
    "This tensor (B, S, H) is what enters the Attention Layer.\n",
    "\n",
    "\n",
    "### Padding index\n",
    "\n",
    "As discussed, we want the vector for the `[PAD]` token (ID 0) to be **strictly zero** so it doesn't affect the math. However, for position embeddings, ID 0 means \"The First Word\", so it *must* be learned.\n",
    "\n",
    "Let's prove that the model freezes the gradient for the Word `[PAD]` but learns the Position `0`.\\\n",
    "We will enable gradients for a quick test, and then inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef5a7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for Word ID 0 ([PAD]):\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "Gradient for Position ID 0 (Start):\n",
      "tensor([-0.1768,  0.1899, -0.4213, -0.4927, -0.2531, -0.4156,  0.2297,  0.4303,\n",
      "         0.5700, -0.0317, -0.3234,  0.7812,  0.7491, -0.3775, -0.3158, -0.1423])\n",
      "\n",
      "✅ Correct: Word ID 0 is frozen (all zeros).\n",
      "✅ Correct: Position 0 is learning.\n"
     ]
    }
   ],
   "source": [
    "embeddings_layer.train()\n",
    "\n",
    "# Create an input containing ID 0 for both Word and Position. Input: Word ID 0 (PAD). Position: implicitly Position 0 (First item)\n",
    "test_input = torch.tensor([[0]]) \n",
    "\n",
    "out = embeddings_layer(test_input)\n",
    "loss = out.abs().sum() # Simple computation to backpropagate loss.\n",
    "loss.backward()\n",
    "\n",
    "grad_word_0 = embeddings_layer.word_embeddings.weight.grad[0]\n",
    "grad_pos_0 = embeddings_layer.position_embeddings.weight.grad[0]\n",
    "\n",
    "print(\"Gradient for Word ID 0 ([PAD]):\")\n",
    "print(grad_word_0)\n",
    "print(\"\\nGradient for Position ID 0 (Start):\")\n",
    "print(grad_pos_0)\n",
    "\n",
    "if torch.sum(torch.abs(grad_word_0)) == 0:\n",
    "    print(\"\\n✅ Correct: Word ID 0 is frozen (all zeros).\")\n",
    "if torch.sum(torch.abs(grad_pos_0)) > 0:\n",
    "    print(\"✅ Correct: Position 0 is learning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
