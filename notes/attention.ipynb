{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "813a6118",
   "metadata": {},
   "source": [
    "# The Attention Mechanism\n",
    "\n",
    "### Query, Key, and Value\n",
    "\n",
    "The attention mechanism is often explained using a **Retrieval System** analogy (like a database or a search engine).\n",
    "\n",
    "For every token in the sequence (e.g., \"bank\"), we generate three vectors:\n",
    "1.  **Query ($Q$)**: *What am I looking for?* (e.g., \"I need context to understand if I mean 'river bank' or 'money bank'.\")\n",
    "2.  **Key ($K$)**: *What do I contain?* (e.g., \"I am the word 'river'.\")\n",
    "3.  **Value ($V$)**: *What information do I pass along?* (e.g., \"I am a nature-related noun.\")\n",
    "\n",
    "\n",
    "**Score**: The Query matches against all Keys. If $Q_{\\text{bank}}$ matches $K_{\\text{river}}$ well, the score is high.\n",
    "\n",
    "This results in a new vector for \"bank\" that is heavily enriched with the concept of \"river\".\n",
    "\n",
    "The formula for Scaled Dot-Product Attention is:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "* $QK^T$: Dot product similarity between Queries and Keys.\n",
    "* $\\sqrt{d_k}$: Scaling factor to prevent gradients from vanishing in the Softmax.\n",
    "* Softmax: Converts raw scores to probabilities summing to 1.\n",
    "\n",
    "### Efficient Multi-Head Attention\n",
    "\n",
    "BERT uses **12 attention heads**. This allows the model to focus on different aspects of language simultaneously (e.g., Head 1 could focus on grammar, Head 2 on vocabulary, Head 3 on sentence structure, etc.).\n",
    "\n",
    "**Naive Implementation (Slow):**\n",
    "Creating 12 separate `nn.Linear` layers and looping over them is inefficient because GPUs prefer large matrix operations over many small ones.\n",
    "\n",
    "**Vectorized Implementation (Fast):**\n",
    "We use **one giant matrix** for all heads and then use tensor reshaping to split them virtually.\n",
    "\n",
    "1.  **Project:** Multiply input $x$ (size 768) by a large weight matrix $W^Q$ (size $768 \\times 768$).\n",
    "2.  **Reshape:** Split the result into 12 chunks of size 64.\n",
    "    * Shape change: `[Batch, Seq, 768]` $\\to$ `[Batch, Seq, 12, 64]`\n",
    "3.  **Permute:** Swap axes so \"Heads\" is its own dimension.\n",
    "    * Shape change: `[Batch, 12, Seq, 64]`\n",
    "\n",
    "Now, a single matrix multiplication `matmul` computes attention for all 12 heads in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
