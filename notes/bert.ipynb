{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b2ac5f",
   "metadata": {},
   "source": [
    "# BERT \n",
    "\n",
    "Before reading this note, make sure you have read the following:\n",
    "* [Embeddings](embeddings.ipynb)\n",
    "* [Attention](attention.ipynb)\n",
    "* [FF](feed_forward.ipynb)\n",
    "* [Encoder](encoder.ipynb)\n",
    "* [Pooler](pooler.ipynb)\n",
    "\n",
    "Up to this point, we have built isolated components. Now, we assemble the **Body** of the Transformer.\n",
    "\n",
    "### Encoder-Only Architecture\n",
    "It is crucial to understand that BERT is an **Encoder-Only** model.\n",
    "* **GPT (Decoder):** Unidirectional. Reads left-to-right. Good at *generating* the future.\n",
    "* **BERT (Encoder):** Bidirectional. Reads the entire sentence at once. Good at *understanding* the context.\n",
    "\n",
    "Because BERT sees the future words, it cannot \"write\" text like GPT. Instead, it creates a deep, contextual \"mental image\" of the text you feed it.\n",
    "\n",
    "### Modules\n",
    "BERT holds three specific sub-modules:\n",
    "\n",
    "1.  **Embeddings:**\n",
    "    * *Input:* Raw integers (Token IDs).\n",
    "    * *Role:* The \"Dictionary.\" Converts discrete IDs into continuous vectors.\n",
    "    * *Analogy:* Looking up words in a massive encyclopedia before reading.\n",
    "\n",
    "2.  **Encoder Stack:**\n",
    "    * *Input:* Vectors from Embeddings.\n",
    "    * *Role:* The \"Brain.\" It loops through N layers of Attention and Feed-Forward processing.\n",
    "    * *Analogy:* Reading the sentence N times, each time understanding the relationships between words better.\n",
    "\n",
    "3.  **Pooler:**\n",
    "    * *Input:* The final state of the `[CLS]` token.\n",
    "    * *Role:* The \"Summary.\" Squeezes the understanding of the whole sequence into one vector.\n",
    "    * *Analogy:* Writing a one-sentence summary of the whole paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a6659",
   "metadata": {},
   "source": [
    "### Data Flow Pipeline\n",
    "\n",
    "When you feed BERT with input IDs, the data travels through a strictly defined pipeline.\n",
    "\n",
    "#### Step 1: Pre-processing \n",
    "The user gives us a mask of `1`s (Keep) and `0`s (Padding).\\\n",
    "The Attention mechanism, however, involves a **Softmax** function:\n",
    "* If we use `0` for padding in the attention scores, $e^0 = 1$, so the model will \"attend\" to the padding a little bit.\n",
    "* We need the value to be $-\\infty$ so that $e^{-\\infty} \\approx 0$, so we make a transformation:\n",
    "$$ \tMask_{new} = (1.0 - Mask_{old}) \\times -10000.0 $$\n",
    "* Input `1` $\to$ `(1-1)*-10k` = **`0`** (Add nothing to the score).\n",
    "* Input `0` $\to$ `(1-0)*-10k` = **`-10000`** (Destroy the score).\n",
    "\n",
    "#### Step 2: Encoder Stack\n",
    "The data enters the Encoder Stack. It passes through Layer 0, then the output of Layer 0 goes to Layer 1, and so on.\n",
    "* **Crucial Property:** The shape never changes.\n",
    "* Input: `(Batch, Seq, 768)` $\to$ Output: `(Batch, Seq, 768)`\n",
    "\n",
    "#### Step 3: Fork\n",
    "At the end, the path splits:\n",
    "1.  **Sequence Output:** The full list of vectors (one for every word).\n",
    "2.  **Pooled Output:** The first token's vector (`[CLS]`) processed by a Tanh layer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
