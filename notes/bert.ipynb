{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b2ac5f",
   "metadata": {},
   "source": [
    "# BERT Backbone\n",
    "\n",
    "Before reading this note, make sure you have read the following:\n",
    "* [Embeddings](embeddings.ipynb)\n",
    "* [Attention](attention.ipynb)\n",
    "* [FF](feed_forward.ipynb)\n",
    "* [Encoder](encoder.ipynb)\n",
    "* [Pooler](pooler.ipynb)\n",
    "\n",
    "Up to this point, we have built isolated components. Now, we assemble the **Body** of the Transformer.\n",
    "\n",
    "### Encoder-Only Architecture\n",
    "It is crucial to understand that BERT is an **Encoder-Only** model.\n",
    "* **GPT (Decoder):** Unidirectional. Reads left-to-right. Good at *generating* the future.\n",
    "* **BERT (Encoder):** Bidirectional. Reads the entire sentence at once. Good at *understanding* the context.\n",
    "\n",
    "Because BERT sees the future words, it cannot \"write\" text like GPT. Instead, it creates a deep, contextual \"mental image\" of the text you feed it.\n",
    "\n",
    "### Modules\n",
    "BERT holds three specific sub-modules:\n",
    "\n",
    "1.  **Embeddings:**\n",
    "    * *Input:* Raw integers (Token IDs).\n",
    "    * *Role:* The \"Dictionary.\" Converts discrete IDs into continuous vectors.\n",
    "    * *Analogy:* Looking up words in a massive encyclopedia before reading.\n",
    "\n",
    "2.  **Encoder Stack:**\n",
    "    * *Input:* Vectors from Embeddings.\n",
    "    * *Role:* The \"Brain.\" It loops through N layers of Attention and Feed-Forward processing.\n",
    "    * *Analogy:* Reading the sentence N times, each time understanding the relationships between words better.\n",
    "\n",
    "3.  **Pooler:**\n",
    "    * *Input:* The final state of the `[CLS]` token.\n",
    "    * *Role:* The \"Summary.\" Squeezes the understanding of the whole sequence into one vector.\n",
    "    * *Analogy:* Writing a one-sentence summary of the whole paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a6659",
   "metadata": {},
   "source": [
    "### Data Flow Pipeline\n",
    "\n",
    "When you feed BERT with input IDs, the data travels through a strictly defined pipeline.\n",
    "\n",
    "#### Step 1: Pre-processing \n",
    "The user gives us a mask of `1`s (Keep) and `0`s (Padding).\\\n",
    "The Attention mechanism, however, involves a **Softmax** function:\n",
    "* If we use `0` for padding in the attention scores, $e^0 = 1$, so the model will \"attend\" to the padding a little bit.\n",
    "* We need the value to be $-\\infty$ so that $e^{-\\infty} \\approx 0$, so we make a transformation:\n",
    "$$ \tMask_{new} = (1.0 - Mask_{old}) \\times -10000.0 $$\n",
    "* Input `1` $\to$ `(1-1)*-10k` = **`0`** (Add nothing to the score).\n",
    "* Input `0` $\to$ `(1-0)*-10k` = **`-10000`** (Destroy the score).\n",
    "\n",
    "#### Step 2: Encoder Stack\n",
    "The data enters the Encoder Stack. It passes through Layer 0, then the output of Layer 0 goes to Layer 1, and so on.\n",
    "* **Crucial Property:** The shape never changes.\n",
    "* Input: `(Batch, Seq, 768)` $\to$ Output: `(Batch, Seq, 768)`\n",
    "\n",
    "#### Step 3: Fork\n",
    "At the end, the path splits:\n",
    "1.  **Sequence Output:** The full list of vectors (one for every word).\n",
    "2.  **Pooled Output:** The first token's vector (`[CLS]`) processed by a Tanh layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c828ce6-f54d-4b29-b067-4c1d6a3fd2bb",
   "metadata": {},
   "source": [
    "# BERT Heads \n",
    "\n",
    "Up to this point, we have built the universal understanding engine: the **Backbone**.\\\n",
    "However, the backbone only outputs vectors. To do anything useful, we need to attach a **Head**.\n",
    "\n",
    "* **Body (Backbone):** Heavy (~110M params). Expensive to train. Generic knowledge.\n",
    "* **Head:** Light (Linear Layers). Cheap to train. Specific task.\n",
    "\n",
    "### Heads\n",
    "These are the specialized modules that plug into the backbone.\n",
    "\n",
    "**A. Masked Language Model (MLM)**\n",
    "* **Goal:** Predict the hidden word (e.g., `[MASK]` $\\to$ \"cat\").\n",
    "* **Input:** `sequence_output` (Vectors for every token).\n",
    "* **Architecture:** `Dense` $\\to$ `GELU` $\\to$ `LayerNorm` $\\to$ `Project to Vocab Size`.\n",
    "* **Crucial Feature:** Weights are tied to the Embeddings to save memory.\n",
    "\n",
    "**B. Next Sentence Prediction (NSP)**\n",
    "* **Goal:** Determine if Sentence B logically follows Sentence A (True/False).\n",
    "* **Input:** `pooled_output` (Summary vector of `[CLS]`).\n",
    "* **Architecture:** `Linear` $\\to$ `2 Classes`.\n",
    "* **Role:** Forces the backbone to learn sentence-level relationships.\n",
    "\n",
    "**C. Sequence Classification**\n",
    "* **Goal:** Classify the entire input text (e.g., Spam vs. Ham).\n",
    "* **Input:** `pooled_output`.\n",
    "* **Architecture:** `Dropout` $\\to$ `Linear` $\\to$ `N Classes`.\n",
    "* **Role:** The standard head used for Fine-Tuning on downstream tasks.\n",
    "\n",
    "\n",
    "### Assembled Models\n",
    "We combine the Backbone and Heads to create the final models used for training.\n",
    "\n",
    "**1. BertForPreTraining**\n",
    "* **Composition:** `Backbone` + `MLM Head` + `NSP Head`.\n",
    "* **Workflow:** Runs both heads simultaneously.\n",
    "    $$\\text{Total Loss} = \\text{Loss}_{\\text{MLM}} + \\text{Loss}_{\\text{NSP}}$$\n",
    "* **Use Case:** Training the model from scratch on large corpora (Wikipedia).\n",
    "\n",
    "**2. BertForSequenceClassification**\n",
    "* **Composition:** `Backbone` + `Classification Head`.\n",
    "* **Workflow:** Loads pre-trained backbone weights, then trains only the specific head.\n",
    "* **Use Case:** Fine-tuning on specific datasets (Sentiment Analysis, Spam Detection, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3f7cf-1c3e-4213-b272-c449c933e2c6",
   "metadata": {},
   "source": [
    "## Weight initialization and gradient clipping\n",
    "\n",
    "When training deep Transformer architectures like BERT from scratch, relying on default deep learning heuristics, such as standard PyTorch weight initialization, frequently leads to catastrophic failure modes like dead neurons, exploding losses, and mode collapse.\n",
    "\n",
    "### Weight initialization variance effect\n",
    "In a standard multi-layer network, PyTorch initializes weights to preserve variance across layers for typical depths. However, in the Masked Language Modeling (MLM) head of Bert, we compute the final logits by taking the dot product of a high-dimensional hidden state vector and the unscaled embedding matrix:\n",
    "$$ \\mathbf{z} = \\mathbf{h} \\mathbf{W}^T $$\n",
    "\n",
    "If $\\mathbf{W}$ (our vocabulary embeddings) is initialized using a standard normal distribution $\\mathcal{N}(0, 1)$, and $\\mathbf{h}$ (the 768-dimensional hidden state) has its own variance, the variance of the resulting logit vector $\\mathbf{z}$ scales linearly with the hidden dimension size $d_{model}$. For $d_{model} = 768$, the standard deviation of our logits becomes massive ($\\approx 27.7$). \n",
    "\n",
    "When these high-variance logits are passed through the Softmax function, the output probability distribution becomes heavily skewed (overconfident) toward a random token.\n",
    "$$ p_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} $$\n",
    "\n",
    "Because the model is highly confident in the *wrong* prediction across a vocabulary of 30,000 tokens, the Cross-Entropy Loss evaluates a near-zero probability for the correct target, yielding a high initial loss ($\\mathcal{L} \\approx 100$).\n",
    "\n",
    "To prevent this immediate saturation, we initialize all linear layers and embeddings using a truncated normal distribution with a tightly constrained standard deviation: $\\mathcal{N}(0, 0.02^2)$. \n",
    "\n",
    "Why 0.02? Mathematically, $1 / \\sqrt{768} \\approx 0.036$. By forcing the variance to be even smaller than $1 / \\sqrt{d_{model}}$, we guarantee that the initial logits $\\mathbf{z}$ are clustered very closely around 0. \n",
    "\n",
    "When logits are near zero, the Softmax function outputs a nearly uniform distribution. For a vocabulary size $|V|$, the probability of any token becomes $p \\approx 1 / |V|$. This yields a predictable, mathematically sound initial loss:\n",
    "$$ \\mathcal{L} = -\\ln\\left(\\frac{1}{|V|}\\right) \\approx 10.31 $$\n",
    "This ensures the model begins training with an open, unbiased state, allowing gradients to flow evenly rather than fighting a massive initial error term.\n",
    "\n",
    "### Gradient clipping\n",
    "Even with perfect initialization, Transformers are notoriously susceptible to early-stage gradient explosions. The self-attention mechanism computes attention scores by multiplying queries and keys. If a single outlier batch produces a sharp attention distribution, it can result in an enormous gradient spike during backpropagation.\n",
    "\n",
    "\n",
    "We implement **Gradient Clipping** to strictly bound the $L_2$ norm of the gradient vector. If the magnitude exceeds our threshold (typically 1.0), we scale the entire gradient vector down. This trick preserves the *direction* of the gradient, ensuring the model still learns the correct pattern, but restricts the *step size*, physically preventing the optimizer from blasting the weights into an irrecoverable dead zone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
