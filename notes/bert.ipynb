{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b2ac5f",
   "metadata": {},
   "source": [
    "# BERT Backbone\n",
    "\n",
    "Before reading this note, make sure you have read the following:\n",
    "* [Embeddings](embeddings.ipynb)\n",
    "* [Attention](attention.ipynb)\n",
    "* [FF](feed_forward.ipynb)\n",
    "* [Encoder](encoder.ipynb)\n",
    "* [Pooler](pooler.ipynb)\n",
    "\n",
    "Up to this point, we have built isolated components. Now, we assemble the **Body** of the Transformer.\n",
    "\n",
    "### Encoder-Only Architecture\n",
    "It is crucial to understand that BERT is an **Encoder-Only** model.\n",
    "* **GPT (Decoder):** Unidirectional. Reads left-to-right. Good at *generating* the future.\n",
    "* **BERT (Encoder):** Bidirectional. Reads the entire sentence at once. Good at *understanding* the context.\n",
    "\n",
    "Because BERT sees the future words, it cannot \"write\" text like GPT. Instead, it creates a deep, contextual \"mental image\" of the text you feed it.\n",
    "\n",
    "### Modules\n",
    "BERT holds three specific sub-modules:\n",
    "\n",
    "1.  **Embeddings:**\n",
    "    * *Input:* Raw integers (Token IDs).\n",
    "    * *Role:* The \"Dictionary.\" Converts discrete IDs into continuous vectors.\n",
    "    * *Analogy:* Looking up words in a massive encyclopedia before reading.\n",
    "\n",
    "2.  **Encoder Stack:**\n",
    "    * *Input:* Vectors from Embeddings.\n",
    "    * *Role:* The \"Brain.\" It loops through N layers of Attention and Feed-Forward processing.\n",
    "    * *Analogy:* Reading the sentence N times, each time understanding the relationships between words better.\n",
    "\n",
    "3.  **Pooler:**\n",
    "    * *Input:* The final state of the `[CLS]` token.\n",
    "    * *Role:* The \"Summary.\" Squeezes the understanding of the whole sequence into one vector.\n",
    "    * *Analogy:* Writing a one-sentence summary of the whole paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a6659",
   "metadata": {},
   "source": [
    "### Data Flow Pipeline\n",
    "\n",
    "When you feed BERT with input IDs, the data travels through a strictly defined pipeline.\n",
    "\n",
    "#### Step 1: Pre-processing \n",
    "The user gives us a mask of `1`s (Keep) and `0`s (Padding).\\\n",
    "The Attention mechanism, however, involves a **Softmax** function:\n",
    "* If we use `0` for padding in the attention scores, $e^0 = 1$, so the model will \"attend\" to the padding a little bit.\n",
    "* We need the value to be $-\\infty$ so that $e^{-\\infty} \\approx 0$, so we make a transformation:\n",
    "$$ \tMask_{new} = (1.0 - Mask_{old}) \\times -10000.0 $$\n",
    "* Input `1` $\to$ `(1-1)*-10k` = **`0`** (Add nothing to the score).\n",
    "* Input `0` $\to$ `(1-0)*-10k` = **`-10000`** (Destroy the score).\n",
    "\n",
    "#### Step 2: Encoder Stack\n",
    "The data enters the Encoder Stack. It passes through Layer 0, then the output of Layer 0 goes to Layer 1, and so on.\n",
    "* **Crucial Property:** The shape never changes.\n",
    "* Input: `(Batch, Seq, 768)` $\to$ Output: `(Batch, Seq, 768)`\n",
    "\n",
    "#### Step 3: Fork\n",
    "At the end, the path splits:\n",
    "1.  **Sequence Output:** The full list of vectors (one for every word).\n",
    "2.  **Pooled Output:** The first token's vector (`[CLS]`) processed by a Tanh layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c828ce6-f54d-4b29-b067-4c1d6a3fd2bb",
   "metadata": {},
   "source": [
    "# BERT Heads \n",
    "\n",
    "Up to this point, we have built the universal understanding engine: the **Backbone**.\\\n",
    "However, the backbone only outputs vectors. To do anything useful, we need to attach a **Head**.\n",
    "\n",
    "* **Body (Backbone):** Heavy (~110M params). Expensive to train. Generic knowledge.\n",
    "* **Head:** Light (Linear Layers). Cheap to train. Specific task.\n",
    "\n",
    "### Heads\n",
    "These are the specialized modules that plug into the backbone.\n",
    "\n",
    "**A. Masked Language Model (MLM)**\n",
    "* **Goal:** Predict the hidden word (e.g., `[MASK]` $\\to$ \"cat\").\n",
    "* **Input:** `sequence_output` (Vectors for every token).\n",
    "* **Architecture:** `Dense` $\\to$ `GELU` $\\to$ `LayerNorm` $\\to$ `Project to Vocab Size`.\n",
    "* **Crucial Feature:** Weights are tied to the Embeddings to save memory.\n",
    "\n",
    "**B. Next Sentence Prediction (NSP)**\n",
    "* **Goal:** Determine if Sentence B logically follows Sentence A (True/False).\n",
    "* **Input:** `pooled_output` (Summary vector of `[CLS]`).\n",
    "* **Architecture:** `Linear` $\\to$ `2 Classes`.\n",
    "* **Role:** Forces the backbone to learn sentence-level relationships.\n",
    "\n",
    "**C. Sequence Classification**\n",
    "* **Goal:** Classify the entire input text (e.g., Spam vs. Ham).\n",
    "* **Input:** `pooled_output`.\n",
    "* **Architecture:** `Dropout` $\\to$ `Linear` $\\to$ `N Classes`.\n",
    "* **Role:** The standard head used for Fine-Tuning on downstream tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Assembled Models\n",
    "We combine the Backbone and Heads to create the final models used for training.\n",
    "\n",
    "**1. BertForPreTraining**\n",
    "* **Composition:** `Backbone` + `MLM Head` + `NSP Head`.\n",
    "* **Workflow:** Runs both heads simultaneously.\n",
    "    $$\\text{Total Loss} = \\text{Loss}_{\\text{MLM}} + \\text{Loss}_{\\text{NSP}}$$\n",
    "* **Use Case:** Training the model from scratch on large corpora (Wikipedia).\n",
    "\n",
    "**2. BertForSequenceClassification**\n",
    "* **Composition:** `Backbone` + `Classification Head`.\n",
    "* **Workflow:** Loads pre-trained backbone weights, then trains only the specific head.\n",
    "* **Use Case:** Fine-tuning on specific datasets (Sentiment Analysis, Spam Detection, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
