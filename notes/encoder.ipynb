{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa595fa",
   "metadata": {},
   "source": [
    "# Encoder Layer \n",
    "\n",
    "Once we have the components: **Embeddings**, **Attention**, and **Feed-Forward** we combine them into the fundamental building block of BERT: the **Encoder Layer**.\n",
    "\n",
    "A single Encoder Layer does two things to the input vectors:\n",
    "1.  **Contextualize:** The Attention mechanism mixes information between tokens (\"Socializing\").\n",
    "2.  **Process:** The Feed-Forward network processes each token individually (\"Thinking\").\n",
    "\n",
    "Crucially, it uses **Residual Connections** (skip connections) and **Layer Normalization** after each step.\n",
    "\n",
    "The most important feature of this layer is that its **Input Shape equals its Output Shape**.\n",
    "* Input: `(Batch, Seq_Len, 768)`\n",
    "* Output: `(Batch, Seq_Len, 768)`\n",
    "\n",
    "This allows us to stack $N$ of them (BERT Base uses 12) without worrying about dimensions mismatching.\n",
    "\n",
    "Let's start by adding src to the python system path in case your notebook is not being run with it already added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac8959e-f697-40ef-8221-360006de21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append((Path('').resolve().parent / 'src').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1cf7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (attention): MultiHeadAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (feed_forward): FeedForwardLayer(\n",
      "    (dense_expansion): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (activation): GELU(approximate='none')\n",
      "    (dense_contraction): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from modules.encoder import Encoder \n",
    "from settings import AttentionSettings, FeedForwardSettings\n",
    "\n",
    "encoder = Encoder(AttentionSettings(),FeedForwardSettings())\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3fa82",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "We will pass a batch of data through the layer.\n",
    "* **Batch:** 2 sentences\n",
    "* **Length:** 10 tokens\n",
    "* **Dim:** 768 (Hidden size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "hidden_size = 768\n",
    "input_tensor = torch.randn(batch_size, seq_len, hidden_size)\n",
    "output = encoder(input_tensor)\n",
    "\n",
    "assert input_tensor.shape == output.shape\n",
    "\n",
    "print(f\"Input Shape:  {input_tensor.shape}\")\n",
    "print(f\"Output Shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0b162-4a03-46ea-a544-7750479f87e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
