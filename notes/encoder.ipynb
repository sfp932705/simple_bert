{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa595fa",
   "metadata": {},
   "source": [
    "# Encoder Layer \n",
    "\n",
    "Once we have the components: **Embeddings**, **Attention**, and **Feed-Forward** we combine them into the fundamental building block of BERT: the **Encoder Layer**.\n",
    "\n",
    "A single Encoder Layer does two things to the input vectors:\n",
    "1.  **Contextualize:** The Attention mechanism mixes information between tokens (\"Socializing\").\n",
    "2.  **Process:** The Feed-Forward network processes each token individually (\"Thinking\").\n",
    "\n",
    "Crucially, it uses **Residual Connections** (skip connections) and **Layer Normalization** after each step.\n",
    "\n",
    "The most important feature of this layer is that its **Input Shape equals its Output Shape**.\n",
    "* Input: `(Batch, Seq_Len, 768)`\n",
    "* Output: `(Batch, Seq_Len, 768)`\n",
    "\n",
    "This allows us to stack $N$ of them (BERT Base uses 12) without worrying about dimensions mismatching.\n",
    "\n",
    "Let's start by adding src to the python system path in case your notebook is not being run with it already added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac8959e-f697-40ef-8221-360006de21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append((Path('').resolve().parent / 'src').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1cf7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (attention): MultiHeadAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (feed_forward): FeedForwardLayer(\n",
      "    (dense_expansion): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (activation): GELU(approximate='none')\n",
      "    (dense_contraction): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from modules.encoders.encoder import Encoder \n",
    "from settings import EncoderSettings\n",
    "\n",
    "encoder = Encoder(EncoderSettings())\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3fa82",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "We will pass a batch of data through the layer.\n",
    "* **Batch:** 2 sentences\n",
    "* **Length:** 10 tokens\n",
    "* **Dim:** 768 (Hidden size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812f89f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([2, 10, 768])\n",
      "Output Shape: torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "hidden_size = 768\n",
    "input_tensor = torch.randn(batch_size, seq_len, hidden_size)\n",
    "output = encoder(input_tensor)\n",
    "\n",
    "assert input_tensor.shape == output.shape\n",
    "\n",
    "print(f\"Input Shape:  {input_tensor.shape}\")\n",
    "print(f\"Output Shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a823963-a37b-45a4-b575-c175c3bb1c29",
   "metadata": {},
   "source": [
    "# Stacked Encoder Layer\n",
    "\n",
    "We have a single Encoder layer. Now we stack them and allow the vectors to pass through multiple layers of Attention and FFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e991cd7e-47b2-4709-a3dc-e6a335eaffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackedEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x Encoder(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForwardLayer(\n",
      "        (dense_expansion): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dense_contraction): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "First Layer: <class 'modules.encoders.encoder.Encoder'>\n",
      "Last Layer:  <class 'modules.encoders.encoder.Encoder'>\n"
     ]
    }
   ],
   "source": [
    "from modules.encoders.layer import StackedEncoder \n",
    "from settings import EncoderSettings\n",
    "\n",
    "stacked_encoder = StackedEncoder(EncoderSettings())\n",
    "print(stacked_encoder)\n",
    "print(f\"\\nFirst Layer: {type(stacked_encoder.layer[0])}\")\n",
    "print(f\"Last Layer:  {type(stacked_encoder.layer[-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c05d04-d54f-4d36-9042-46dda32f26c4",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43daa613-cc0f-40a2-8830-b2003a503d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([2, 10, 768])\n",
      "Output Shape: torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "output = stacked_encoder(input_tensor)\n",
    "\n",
    "assert input_tensor.shape == output.shape\n",
    "\n",
    "print(f\"Input Shape:  {input_tensor.shape}\")\n",
    "print(f\"Output Shape: {output.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
