class RustBPETokenizer:
    def __init__(
        self, vocab_size: int, special_tokens: list[str], unused_tokens: int
    ) -> None: ...
    def train(self, corpus: str) -> None: ...
    def encode(self, text: str) -> list[int]: ...
    def decode(self, ids: list[int]) -> str: ...
    def get_vocab(self) -> dict[str, int]: ...
    def get_merges(self) -> list[tuple[str, str]]: ...

class RustWordPieceTokenizer:
    def __init__(
        self, vocab_size: int, special_tokens: list[str], unused_tokens: int
    ) -> None: ...
    def train(self, corpus: str) -> None: ...
    def encode(self, text: str) -> list[int]: ...
    def decode(self, ids: list[int]) -> str: ...
    def get_vocab(self) -> dict[str, int]: ...
